{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b2fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a2c22e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 644/644 [00:13<00:00, 48.05it/s]\n",
      "100%|███████████████████████████████████████| 2587/2587 [00:56<00:00, 46.02it/s]\n",
      "100%|█████████████████████████████████████████| 663/663 [00:13<00:00, 48.11it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "file_list = glob.glob('/home/user/Helipad_Detection/workspace/annotations/stanford_raw/*.csv')\n",
    "\n",
    "for file in file_list:\n",
    "    t = file.split('/')[-1].split('_')[0]\n",
    "    \n",
    "    image_path = f'/home/user/Helipad_Detection/workspace/images/stanford/{t}'\n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    before_cols = ['filename','xmin','ymin','xmax','ymax', 'class']\n",
    "    data.columns = before_cols\n",
    "    data['filename'] = data['filename'].apply(lambda x : x.split('/')[-1])\n",
    "    \n",
    "    size_dic = {}\n",
    "    for fnm in tqdm.tqdm(data['filename'].unique()):\n",
    "        height, width, _= cv2.imread(os.path.join(image_path, fnm)).shape\n",
    "        size_dic[fnm] = (height, width)\n",
    "        \n",
    "    data['height'] = list(map(lambda x: size_dic[x][0], data['filename']))\n",
    "    data['width'] = list(map(lambda x: size_dic[x][1], data['filename']))\n",
    "    \n",
    "    after_cols = ['filename','width','height','class','xmin','ymin','xmax','ymax']\n",
    "    data[after_cols].to_csv('/home/user/Helipad_Detection/workspace/annotations/' + file.split('/')[-1], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2b28122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import io\n",
    "import xml.etree.ElementTree as ET\n",
    "import argparse\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\n",
    "import tensorflow.compat.v1 as tf\n",
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util, label_map_util\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6919b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_text_to_int(row_label):\n",
    "    return label_map_dict[row_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b79fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df, group):\n",
    "    data = namedtuple('data', ['filename', 'object'])\n",
    "    gb = df.groupby(group)\n",
    "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a439157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_example(group, path):\n",
    "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "\n",
    "    filename = group.filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    for index, row in group.object.iterrows():\n",
    "        xmins.append(row['xmin'] / width)\n",
    "        xmaxs.append(row['xmax'] / width)\n",
    "        ymins.append(row['ymin'] / height)\n",
    "        ymaxs.append(row['ymax'] / height)\n",
    "        classes_text.append(row['class'].encode('utf8'))\n",
    "        classes.append(class_text_to_int(row['class']))\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a753aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob('/home/user/Helipad_Detection/workspace/annotations/*_stanford.csv')\n",
    "path = '/home/user/Helipad_Detection/workspace/images/stanford'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cae0ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap('/home/user/Helipad_Detection/workspace/annotations/label_map.pbtxt')\n",
    "label_map_dict = label_map_util.get_label_map_dict(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b215c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the TFRecord file: /home/user/Helipad_Detection/workspace/annotations/train_stanford.record\n",
      "Successfully created the TFRecord file: /home/user/Helipad_Detection/workspace/annotations/val_stanford.record\n",
      "Successfully created the TFRecord file: /home/user/Helipad_Detection/workspace/annotations/test_stanford.record\n"
     ]
    }
   ],
   "source": [
    "for t in ['train', 'val', 'test']:\n",
    "    file = f'/home/user/Helipad_Detection/workspace/annotations/{t}_stanford.csv'\n",
    "    path = f'/home/user/Helipad_Detection/workspace/images/stanford/{t}'\n",
    "    output_path = f'/home/user/Helipad_Detection/workspace/annotations/{t}_stanford.record'\n",
    "    writer = tf.python_io.TFRecordWriter(output_path)\n",
    "    \n",
    "    examples = pd.read_csv(file)\n",
    "    grouped = split(examples, 'filename')\n",
    "    for group in grouped:\n",
    "        tf_example = create_tf_example(group, path)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()\n",
    "    print('Successfully created the TFRecord file: {}'.format(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30375f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:heli-py37-tf2] *",
   "language": "python",
   "name": "conda-env-heli-py37-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
